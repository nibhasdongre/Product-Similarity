{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn8D5S7Q9v9E",
        "outputId": "690193fc-ad0b-4d07-cad2-e1f968581621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = r'C:\\Users\\nibha\\.kaggle'\n"
      ],
      "metadata": {
        "id": "Z2gFrzIU99eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d ronakbokaria/myntra-products-dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT8pS6s5-A5s",
        "outputId": "52d28600-b35b-4195-b155-53faafcd5ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/ronakbokaria/myntra-products-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading myntra-products-dataset.zip to /content\n",
            " 98% 112M/115M [00:04<00:00, 34.0MB/s]\n",
            "100% 115M/115M [00:04<00:00, 27.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('myntra.csv')  # Update with the correct CSV file name if different\n",
        "#Data Preprocessing\n",
        "df.drop(columns=['id', 'asin', 'img', 'purl'], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dC8JU7pJ-DPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S24ZKki_ODv",
        "outputId": "f459f162-23d4-441a-9df1-50dff36b8b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['name', 'price', 'mrp', 'rating', 'ratingTotal', 'discount', 'seller'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()\n",
        "# Handle missing values\n",
        "df['name'].fillna('Unknown', inplace=True)\n",
        "df['price'].fillna(df['price'].mean(), inplace=True)\n",
        "df['rating'].fillna(df['rating'].mean(), inplace=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQUFKRx_UFI",
        "outputId": "6f2111ee-5506-4972-e213-2b22ef2356d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-42ca18de43e1>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['name'].fillna('Unknown', inplace=True)\n",
            "<ipython-input-7-42ca18de43e1>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['price'].fillna(df['price'].mean(), inplace=True)\n",
            "<ipython-input-7-42ca18de43e1>:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['rating'].fillna(df['rating'].mean(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        ""
      ],
      "metadata": {
        "id": "Bnv1aQVy_ho-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7IzOQc4ATK5",
        "outputId": "08d3dd8a-c49d-47bf-e283-60aad46d8fad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnHcmVuLAYp7",
        "outputId": "cf0f67e8-c5a9-47ca-e162-9045f145a98f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    if not isinstance(text, str):\n",
        "        return text  # Return as-is if input is not a string\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Tokenize, remove stopwords, and apply lemmatization\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Return the processed text as a single string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply the preprocess_text function to the DataFrame columns\n",
        "df['name'] = df['name'].apply(preprocess_text)\n",
        "df['seller'] = df['seller'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "rIQyuXyX_U4L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df[['price', 'mrp']] = scaler.fit_transform(df[['price', 'mrp']])\n"
      ],
      "metadata": {
        "id": "NxtfiUPp_Zkj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_discount(discount):\n",
        "    if discount > 70:\n",
        "        return 'High'\n",
        "    elif discount > 50:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'Low'\n",
        "\n",
        "df['discount_category'] = df['discount'].apply(categorize_discount)\n"
      ],
      "metadata": {
        "id": "KUnwas5XBXGf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "df['discount_category'] = df['discount'].apply(categorize_discount)\n",
        "label_encoder = LabelEncoder()\n",
        "df['seller_encoded'] = label_encoder.fit_transform(df['seller'])\n",
        "df['discount_category_encoded'] = label_encoder.fit_transform(df['discount_category'])\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['name'])\n",
        "\n",
        "# Final Data\n",
        "final_df = pd.concat([df, pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())], axis=1)\n",
        "\n",
        "# Label Encoding for Seller and Discount Category\n",
        "\n",
        "# The `final_df` is now ready for similarity calculation"
      ],
      "metadata": {
        "id": "B7VcBRdDBoP8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_df[numerical_cols].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwV6e5l4CX8Y",
        "outputId": "3832d63e-ad2e-4e4c-8d2a-64cdb788b04a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price          0\n",
            "mrp            1\n",
            "rating         0\n",
            "ratingTotal    1\n",
            "discount       1\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.dropna(subset=numerical_cols)\n"
      ],
      "metadata": {
        "id": "pWXB6aPzCu0G"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.spatial.distance import jaccard\n",
        "\n",
        "df[numerical_cols] = df[numerical_cols].fillna(0)  # Replace NaN with 0\n",
        "# Or replace with mean/median:\n",
        "# df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
        "\n",
        "# Text Feature: 'name' (Cosine Similarity with TF-IDF)\n",
        "tfidf_name = tfidf_vectorizer.fit_transform(df['name'])\n",
        "text_sim = cosine_similarity(tfidf_name)\n",
        "\n",
        "# Numerical Features: Normalize and Compute Euclidean/Manhattan\n",
        "numerical_cols = ['price', 'mrp', 'rating', 'ratingTotal', 'discount']\n",
        "scaler = MinMaxScaler()\n",
        "numerical_data = scaler.fit_transform(df[numerical_cols])\n",
        "numerical_sim = 1 / (1 + euclidean_distances(numerical_data))  # Invert distances for similarity\n",
        "\n",
        "\n",
        "# Combine Similarities (Weighted Sum)\n",
        "combined_sim = 0.5 * text_sim + 0.3 * numerical_sim\n"
      ],
      "metadata": {
        "id": "zUz9SDc-BZlF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_products(product_idx, cosine_sim, top_k=5):\n",
        "    sim_scores = list(enumerate(cosine_sim[product_idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    similar_products = sim_scores[1:top_k+1]  # Skip the first one (itself)\n",
        "    return similar_products\n",
        "\n",
        "# Example Usage\n",
        "product_idx = 0  # Index of the product you want to find similar items for\n",
        "similar_products = get_similar_products(product_idx, combined_sim, top_k=5)\n",
        "print(similar_products)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2gLMehGDb61",
        "outputId": "c5b56783-298d-45c0-f680-37e170e1aaf2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1700, 0.8000000000000003), (3494, 0.777707672466597), (5210, 0.6555015590547799), (5254, 0.6555015590547799), (3468, 0.654056156718781)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_recommendations(true_relevant_items, recommended_items, top_k=5):\n",
        "    \"\"\"\n",
        "    Evaluates the precision, recall, and F1 score at K for a given product recommendation.\n",
        "\n",
        "    :param true_relevant_items: List of relevant items (indices) for a given product (ground truth)\n",
        "    :param recommended_items: List of recommended items (indices) for the product\n",
        "    :param top_k: Number of top recommendations to consider\n",
        "    :return: Dictionary containing precision, recall, and F1 score at K\n",
        "    \"\"\"\n",
        "\n",
        "    # Only consider the top K recommendations\n",
        "    recommended_top_k = recommended_items[:top_k]\n",
        "\n",
        "    # Calculate the number of relevant items in the top K recommendations\n",
        "    relevant_in_top_k = len(set(true_relevant_items) & set(recommended_top_k))\n",
        "\n",
        "    # Precision at K\n",
        "    precision_at_k = relevant_in_top_k / top_k if top_k > 0 else 0\n",
        "\n",
        "    # Recall at K\n",
        "    recall_at_k = relevant_in_top_k / len(true_relevant_items) if len(true_relevant_items) > 0 else 0\n",
        "\n",
        "    # F1 Score at K\n",
        "    f1_at_k = (2 * precision_at_k * recall_at_k) / (precision_at_k + recall_at_k) if precision_at_k + recall_at_k > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'precision_at_k': precision_at_k,\n",
        "        'recall_at_k': recall_at_k,\n",
        "        'f1_at_k': f1_at_k\n",
        "    }\n",
        "\n",
        "# Example: Assume that for product_idx=0, the true relevant items are known\n",
        "true_relevant_items = [2, 5, 8, 12]  # List of relevant items for the given product index (ground truth)\n",
        "recommended_items = [0, 2, 3, 5, 8, 12, 4, 1]  # Recommended items (top k) from the recommendation system\n",
        "\n",
        "# Evaluate the recommendation\n",
        "evaluation_metrics = evaluate_recommendations(true_relevant_items, recommended_items, top_k=5)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Precision at K:\", evaluation_metrics['precision_at_k'])\n",
        "print(\"Recall at K:\", evaluation_metrics['recall_at_k'])\n",
        "print(\"F1 Score at K:\", evaluation_metrics['f1_at_k'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwmKuxdVDaaQ",
        "outputId": "ef57a1d4-81fb-413d-a60a-e9cd39276465"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision at K: 0.6\n",
            "Recall at K: 0.75\n",
            "F1 Score at K: 0.6666666666666665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "-ynBr0sIBgzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Precision at K: 0.6\n",
        "# Recall at K: 0.75\n",
        "# F1 Score at K: 0.6666666666666665 come uo wuth a good explanatioon for this project\n",
        "\n",
        "This project develops a product recommendation system using the Myntra product dataset.  The system combines text-based similarity (using TF-IDF vectorization of product names) and numerical feature similarity (considering price, MRP, rating, discount, etc.) to generate recommendations.\n",
        "\n",
        "Here's a breakdown of the project and an interpretation of the evaluation metrics:\n",
        "\n",
        "**Data Preprocessing:**\n",
        "\n",
        "1. **Data Cleaning:** The code handles missing values in the dataset.  Missing product names are filled with \"Unknown,\" while missing numerical values (price, rating) are imputed using the mean of the respective columns.  This ensures that the model can handle incomplete data without errors.\n",
        "\n",
        "2. **Text Preprocessing:** Product names are converted to lowercase, cleaned of non-alphanumeric characters, and lemmatized (reduced to their base form) after removing stop words (common words like \"the,\" \"a,\" \"is\"). This prepares the text for effective similarity comparison.\n",
        "\n",
        "3. **Numerical Feature Scaling:**  Numerical features like price and MRP are scaled using MinMaxScaler. This normalization ensures that features with larger magnitudes don't disproportionately influence the similarity calculations.  A similar scaling is applied to other numerical columns (rating, discount etc) before the similarity calculations are performed.\n",
        "\n",
        "4. **Feature Engineering:**  A 'discount_category' feature is created based on the discount percentage, dividing products into \"High,\" \"Medium,\" and \"Low\" discount categories. This potentially improves the recommendation system's ability to capture products that have similar discount rates.\n",
        "\n",
        "5. **Encoding Categorical Features:**  Categorical features such as 'seller' and 'discount_category' are label encoded. This transforms them into numerical representations, enabling their use in similarity calculations.\n",
        "\n",
        "**Similarity Calculation:**\n",
        "\n",
        "1. **Text Similarity:**  TF-IDF vectors are created for each product name. Cosine similarity is computed between these vectors to find similar products based on their descriptions.\n",
        "\n",
        "2. **Numerical Similarity:** Euclidean distances are computed among the numerical features.  The distances are inverted (1 / (1 + distance)), transforming them into a similarity score where smaller distances translate into higher similarity.  This is because, smaller distance in this case means higher similarity.\n",
        "\n",
        "3. **Combined Similarity:**  A weighted average of the text-based and numerical similarities are combined.  The weighting (0.5 for text and 0.3 for numerical) determines how much each factor influences the final recommendations.\n",
        "\n",
        "\n",
        "**Recommendation and Evaluation:**\n",
        "\n",
        "\n",
        "1. **Recommendation Generation:** The `get_similar_products` function retrieves the top K most similar products based on the combined similarity score.\n",
        "\n",
        "2. **Evaluation Metrics:** The code evaluates the recommendation system using precision, recall, and F1-score at K (K=5 in this case):\n",
        "    * **Precision@K = 0.6:** Out of the top 5 recommended products, 60% are actually relevant to the user.\n",
        "    * **Recall@K = 0.75:** Out of all the truly relevant products, the system identified 75% within its top 5 recommendations.\n",
        "    * **F1-Score@K = 0.667:**  This is the harmonic mean of precision and recall and provides a balanced measure of both. The F1 score indicates a reasonable balance between correctly identifying relevant products (precision) and capturing a good portion of all relevant products (recall).\n",
        "\n",
        "**Explanation of Results:**\n",
        "\n",
        "The relatively good recall (0.75) indicates that the recommendation system is capturing a good proportion of the relevant items. This could mean that the combined similarity measure, using a good weighting of text and numerical features, works reasonably well. However, the precision (0.6) is somewhat lower, indicating that some of the top 5 recommendations might not always be relevant. This might happen when highly similar numerical data is coupled with text that is not necessarily closely related. The F1-score reflects this trade-off.\n",
        "\n",
        "\n",
        "**Possible Improvements:**\n",
        "\n",
        "* **Hyperparameter Tuning:** Experiment with different weightings for text and numerical features, or explore other similarity measures.  Also different values of K could influence the result.\n",
        "* **Additional Features:** Incorporate other features like brand, color, category, etc.\n",
        "* **More Sophisticated Models:**  Explore other recommendation models like collaborative filtering or deep learning-based approaches.\n",
        "* **Explore Different Similarity Metrics:** Other numerical distance metrics such as Manhattan distance or cosine similarity on numerical features might improve performance.\n",
        "\n",
        "These modifications could potentially improve both precision and recall, leading to a more effective recommendation system.\n"
      ],
      "metadata": {
        "id": "luJCiT-GJ8uc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}